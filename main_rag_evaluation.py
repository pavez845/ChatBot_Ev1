# Sistema RAG MÃ©dico - Hospital Barros Luco, Santiago de Chile
# VersiÃ³n: 3.0 - Adaptado para asistencia mÃ©dica y gestiÃ³n hospitalaria
import streamlit as st
import os
import json
import time
import uuid
from datetime import datetime
import pandas as pd
import numpy as np
from openai import OpenAI
from sklearn.metrics.pairwise import cosine_similarity
import plotly.express as px
import plotly.graph_objects as go

# LangChain imports
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationSummaryBufferMemory#Mantiene los mensajes recientes completos y resume conversaciones antiguas para ahorrar tokens
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Archivo: main_rag_evaluation.py
# Sistema RAG MÃ©dico con EvaluaciÃ³n Avanzada

# LibrerÃ­as bÃ¡sicas
import streamlit as st
import time
import uuid
from datetime import datetime
from collections import Counter
import os
import openai
from openai import OpenAI

# Importaciones de LangChain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.memory import ConversationSummaryBufferMemory
from langchain.schema import Document

# LibrerÃ­as para anÃ¡lisis y visualizaciÃ³n
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from sklearn.metrics.pairwise import cosine_similarity

# Cargar variables de entorno desde archivo .env
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    st.warning("âš  python-dotenv no estÃ¡ instalado. InstÃ¡lalo con: pip install python-dotenv")

# variables de entorno :)
github_token = os.getenv("GITHUB_TOKEN")
github_base_url = os.getenv("GITHUB_BASE_URL", "https://models.inference.ai.azure.com")
LANGSMITH_TRACING = os.getenv("LANGSMITH_TRACING", "true")
LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY")
LANGSMITH_PROJECT = os.getenv("LANGSMITH_PROJECT", "hospital-barros-luco")

# Configuracion de LangSmith si estÃ¡ disponible
if LANGSMITH_TRACING.lower() == "true" and LANGSMITH_API_KEY:
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_API_KEY"] = LANGSMITH_API_KEY
    os.environ["LANGCHAIN_PROJECT"] = LANGSMITH_PROJECT

if github_token:
    os.environ["OPENAI_API_KEY"] = github_token
    os.environ["OPENAI_API_BASE"] = github_base_url
else:
    st.error("âŒ GITHUB_TOKEN environment variable is not set. Please check your .env file.")
    st.info("ğŸ’¡ Make sure your .env file contains: GITHUB_TOKEN=your_token_here")
    st.stop()

st.set_page_config(page_title="ğŸ¥ Hospital Barros Luco - Asistente IA", page_icon="ğŸ¥", layout="wide")  # Configura pÃ¡gina web del hospital

class ChatbotMedicoRAG:  # Clase principal del chatbot mÃ©dico
    def __init__(self):
        self.client = None  # Cliente OpenAI para consultas mÃ©dicas
        self.llm = None  # Modelo de lenguaje especializado en medicina
        self.embeddings_model = None  # Modelo para crear vectores de informaciÃ³n mÃ©dica
        self.vectorstore = None  # Base de datos vectorial de conocimiento mÃ©dico
        self.conversation_chain = None  # Cadena conversacional para consultas mÃ©dicas
        self.memory = None  # Sistema de memoria para historial de consultas
        self.documents = []  # Lista de documentos mÃ©dicos y protocolos
        self.embeddings = None  # Vectores generados de documentos mÃ©dicos
        self.interaction_logs = []  # Registro de todas las consultas mÃ©dicas
        
    def initialize_client(self):
        """Inicializa el cliente de OpenAI"""
        if not github_token:
            st.error("âŒ GitHub token not available")
            return False
        
        try:
            self.client = OpenAI(  # Crea cliente OpenAI directo
                base_url=github_base_url,
                api_key=github_token
            )
            return True
        except Exception as e:
            st.error(f"Error initializing client: {str(e)}")
            return False

    def initialize_llm(self):
        """Inicializa el modelo de lenguaje de LangChain"""
        try:
            self.llm = ChatOpenAI(  # Configura modelo GPT-4o-mini con temperatura baja
                model="gpt-4o-mini",
                temperature=0.1,  # Temperatura baja = respuestas mÃ¡s serio
                openai_api_key=github_token,
                openai_api_base=github_base_url
            )
            return True
        except Exception as e:
            st.error(f"Error initializing LLM: {str(e)}")
            return False

    def initialize_embeddings(self):
        """Inicializa el modelo de embeddings de LangChain"""
        if not github_token:
            st.error("âŒ GitHub token not available for embeddings")
            return False
        
        try:
            self.embeddings_model = OpenAIEmbeddings(  # Modelo para convertir texto en vectores
                model="text-embedding-3-small",  # Modelo pequeÃ±o pero eficiente
                openai_api_key=github_token,
                openai_api_base=github_base_url
            )
            return True
        except Exception as e:
            st.error(f"Error initializing embeddings: {str(e)}")
            return False

    def setup_memory(self):
        """Configura la memoria conversacional"""
        try:
            self.memory = ConversationSummaryBufferMemory(  # Memoria hÃ­brida mÃ©dica
                llm=self.llm,
                max_token_limit=2000,  # LÃ­mite de tokens para evitar contexto muy largo
                memory_key="chat_history",  # Clave para acceder al historial
                return_messages=True,  # Devuelve mensajes estructurados
                output_key="answer"  # Clave para las respuestas del asistente
            )
            return True
        except Exception as e:
            st.error(f"Error setting up memory: {str(e)}")
            return False

    def get_embeddings_langchain(self, texts):
        """Obtiene embeddings usando LangChain"""
        try:
            if isinstance(texts[0], str):  # Si son strings, convertir a documentos
                documents = [Document(page_content=text) for text in texts]
            else:
                documents = texts
            
            embeddings = self.embeddings_model.embed_documents([doc.page_content for doc in documents])  # Convierte textos en vectores
            return np.array(embeddings)  # Devuelve como array numpy
        except Exception as e:
            st.error(f"Error getting embeddings: {str(e)}")
            return None

    def get_query_embedding_langchain(self, query):
        """Obtiene embedding de consulta usando LangChain"""
        try:
            embedding = self.embeddings_model.embed_query(query)  # Convierte pregunta en vector
            return np.array(embedding)
        except Exception as e:
            st.error(f"Error getting query embedding: {str(e)}")
            return None

    def hybrid_search_with_metrics(self, query, top_k=5):
        """Realiza bÃºsqueda hÃ­brida con mÃ©tricas detalladas"""
        if not self.embeddings_model or not self.documents or self.embeddings is None:
            return [], 0.0
            
        start_time = time.time()  # Inicia cronÃ³metro para mÃ©tricas
        
        # Get query embedding
        query_embedding = self.get_query_embedding_langchain(query)  # Convierte pregunta a vector
        if query_embedding is None:
            return [], 0.0
        
        # Semantic similarity
        semantic_similarities = cosine_similarity([query_embedding], self.embeddings)[0]  # Calcula similitud semÃ¡ntica (70%)
        
        # Keyword similarity
        keyword_scores = []  # Lista para almacenar puntuaciones de palabras clave
        query_words = set(query.lower().split())  # Convierte pregunta en conjunto de palabras
        for doc in self.documents:
            doc_words = set(doc.lower().split())  # Convierte documento en conjunto de palabras
            overlap = len(query_words.intersection(doc_words))  # Cuenta palabras en comÃºn
            keyword_scores.append(overlap / max(len(query_words), 1))  # Calcula porcentaje de coincidencia
        
        # Combined score
        combined_scores = 0.7 * semantic_similarities + 0.3 * np.array(keyword_scores)  # 70% semÃ¡ntica + 30% keywords
        top_indices = np.argsort(combined_scores)[::-1][:top_k]  # Obtiene Ã­ndices de mejores resultados
        
        results = []
        for idx in top_indices:
            results.append({  # Crea diccionario con toda la informaciÃ³n del resultado
                'document': self.documents[idx],
                'semantic_score': semantic_similarities[idx],
                'keyword_score': keyword_scores[idx],
                'combined_score': combined_scores[idx],
                'index': idx
            })
        
        retrieval_time = time.time() - start_time  # Calcula tiempo total de bÃºsqueda
        return results, retrieval_time

    def generate_response_with_metrics(self, query, context_docs):
        """Genera respuesta con mÃ©tricas de tiempo y filtrado adecuado de fuentes"""
        if not self.client:
            return "Error: Cliente no disponible", 0.0
            
        start_time = time.time()  # Inicia cronÃ³metro
        
        # Filtra documentos de contexto relevantes
        relevant_docs = []  # Lista para documentos realmente relevantes
        query_words = set(query.lower().split())  # Palabras de la pregunta
        stop_words = {'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'es', 'se', 'no', 'te', 'lo', 'le', 'da', 'su', 'por', 'son', 'con', 'para', 'como', 'quÃ©', 'cÃ³mo', 'cuÃ¡l', 'hola', 'gracias'}  # Palabras sin significado
        meaningful_words = query_words - stop_words  # Solo palabras importantes
        
        for doc in context_docs:
            content = doc['document'].lower()
            # Verifica si el documento es realmente relevante
            if len(meaningful_words) > 0:  # Si hay palabras importantes
                content_words = set(content.split())
                if meaningful_words.intersection(content_words):  # Si encuentra palabras importantes en el documento
                    relevant_docs.append(doc)  # Agrega documento a la lista relevante
            elif len(query_words) <= 2:  # Very short queries like "hola"
                continue  # Don't use any context for greetings
        
        # Decide whether to use context or general knowledge
        if relevant_docs:  # Si hay documentos relevantes
            context = "\n".join([f"Documento {i+1}: {doc['document']}" 
                                for i, doc in enumerate(relevant_docs)])  # Crea contexto con documentos relevantes
            
            prompt = f"""Eres el asistente virtual del Hospital Barros Luco en Santiago de Chile. 

Contexto mÃ©dico e informaciÃ³n del hospital:
{context}

Consulta del paciente o visitante: {query}

Proporciona una respuesta clara, empÃ¡tica y basada en los protocolos del hospital. 
Si es una emergencia mÃ©dica, recuerda al usuario que debe dirigirse inmediatamente al Ã¡rea de Urgencias."""  # Prompt mÃ©dico hospitalario
        else:
            # No relevant context found
            prompt = f"""Eres el asistente virtual del Hospital Barros Luco en Santiago de Chile.

Consulta: {query}

ğŸ¥ BasÃ¡ndome en conocimiento mÃ©dico general (esta informaciÃ³n no estÃ¡ en nuestros protocolos especÃ­ficos): 

Proporciona informaciÃ³n Ãºtil pero siempre recuerda al usuario que debe consultar con personal mÃ©dico del hospital para diagnÃ³sticos o tratamientos especÃ­ficos."""  # Prompt mÃ©dico general

        try:
            # Crea solicitud de completado a OpenAI
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",    # Modelo de OpenAI a usar
                messages=[{"role": "user", "content": prompt}],  # Mensaje para la IA
                temperature=0.1,        # Creatividad baja para respuestas precisas
                max_tokens=600          # LÃ­mite de palabras en respuesta
            )
            
            generation_time = time.time() - start_time  # Calcula tiempo total de generaciÃ³n
            response_text = response.choices[0].message.content  # Extrae texto de respuesta
            
            # Add prefix if using general knowledge
            if not relevant_docs:  # Si no usÃ³ base de datos
                response_text = f"ğŸ§  BasÃ¡ndome en conocimiento general (no encontrÃ© informaciÃ³n especÃ­fica en la base de datos): {response_text}"  # Agrega prefijo indicativo
            
            return response_text, generation_time  # Retorna respuesta y tiempo
        except Exception as e:
            return f"Error generating response: {str(e)}", time.time() - start_time  # Retorna error si algo falla

    def evaluate_faithfulness(self, query, context, response):
        """EvalÃºa la fidelidad de la respuesta al contexto"""
        if not self.client:  # Verifica que el cliente estÃ© disponible
            return 5.0
            
        # Crea prompt de evaluaciÃ³n para fidelidad
        eval_prompt = f"""EvalÃºa si la respuesta es fiel al contexto proporcionado.

Consulta: {query}

Contexto:
{context}

Respuesta:
{response}

Â¿La respuesta estÃ¡ basada Ãºnicamente en la informaciÃ³n del contexto? 
Responde con un nÃºmero del 1-10 donde:
- 1-3: Respuesta contradice o no estÃ¡ basada en el contexto
- 4-6: Respuesta parcialmente basada en el contexto
- 7-10: Respuesta completamente fiel al contexto

Responde SOLO con el nÃºmero:"""  # Prompt especÃ­fico para evaluar fidelidad

        try:
            # Genera evaluaciÃ³n usando OpenAI
            result = self.client.chat.completions.create(
                model="gpt-4o-mini",      # Modelo para evaluaciÃ³n
                messages=[{"role": "user", "content": eval_prompt}],  # Prompt de evaluaciÃ³n
                temperature=0.1,          # Baja creatividad para evaluaciÃ³n consistente
                max_tokens=10             # Solo necesita un nÃºmero
            )
            return float(result.choices[0].message.content.strip())  # Convierte respuesta a nÃºmero
        except:
            return 5.0  # Valor por defecto si hay error

    def evaluate_relevance(self, query, response):
        """EvalÃºa la relevancia de la respuesta a la consulta"""
        if not self.client:  # Verifica que el cliente estÃ© disponible
            return 5.0
            
        # Crea prompt de evaluaciÃ³n para relevancia
        eval_prompt = f"""EvalÃºa quÃ© tan relevante es la respuesta para la consulta.

Consulta: {query}

Respuesta: {response}

Â¿QuÃ© tan bien responde la respuesta a la consulta?
Responde con un nÃºmero del 1-10 donde:
- 1-3: Respuesta no relacionada o irrelevante
- 4-6: Respuesta parcialmente relevante
- 7-10: Respuesta muy relevante y Ãºtil

Responde SOLO con el nÃºmero:"""  # Prompt especÃ­fico para evaluar relevancia

        try:
            # Genera evaluaciÃ³n usando OpenAI
            result = self.client.chat.completions.create(
                model="gpt-4o-mini",      # Modelo para evaluaciÃ³n
                messages=[{"role": "user", "content": eval_prompt}],  # Prompt de evaluaciÃ³n
                temperature=0.1,          # Baja creatividad para consistencia
                max_tokens=10             # Solo necesita un nÃºmero
            )
            return float(result.choices[0].message.content.strip())  # Convierte respuesta a nÃºmero
        except:
            return 5.0  # Valor por defecto si hay error

    def evaluate_context_precision(self, query, retrieved_docs):
        """EvalÃºa la precisiÃ³n del contexto"""
        if not self.client or not retrieved_docs:  # Verifica cliente y documentos
            return 0.0
        
        relevant_count = 0  # Contador de documentos relevantes
        for doc in retrieved_docs:  # EvalÃºa cada documento recuperado
            # Crea prompt de evaluaciÃ³n para cada documento
            eval_prompt = f"""Â¿Este documento es relevante para responder la consulta?

Consulta: {query}

Documento: {doc['document'][:300]}...

Responde SOLO 'SI' o 'NO':"""  # Prompt simple para evaluar relevancia del documento
            
            try:
                # Evaluate document relevance
                result = self.client.chat.completions.create(
                    model="gpt-4o-mini",      # Modelo para evaluaciÃ³n
                    messages=[{"role": "user", "content": eval_prompt}],  # Prompt de evaluaciÃ³n
                    temperature=0.1,          # Baja creatividad para respuestas consistentes
                    max_tokens=5              # Solo necesita SI/NO
                )
                if result.choices[0].message.content.strip().upper() == 'SI':  # Si el documento es relevante
                    relevant_count += 1  # Incrementa contador
            except:
                pass  # Ignora errores individuales
        
        return relevant_count / len(retrieved_docs)  # Retorna precisiÃ³n (% de docs relevantes)

    def log_interaction(self, query, response, metrics, context_docs):
        """Registra la interacciÃ³n para anÃ¡lisis"""
        # Crea entrada de registro con todos los datos de interacciÃ³n
        log_entry = {
            'id': str(uuid.uuid4()),  # ID Ãºnico para esta interacciÃ³n
            'timestamp': datetime.now().isoformat(),  # Marca de tiempo actual
            'query': query,                           # Pregunta del usuario
            'response': response,                     # Respuesta generada
            'metrics': metrics,                       # MÃ©tricas de calidad
            'context_count': len(context_docs),       # NÃºmero de documentos usados
            'context_scores': [doc.get('combined_score', 0) for doc in context_docs]  # Puntuaciones de relevancia
        }
        
        self.interaction_logs.append(log_entry)  # Agrega al historial de interacciones

    def clean_placeholder_documents(self):
        """Elimina documentos de prueba y placeholder del almacÃ©n vectorial"""
        if not self.vector_store:  # Verifica que existe el vector store
            return False
            
        try:
            # Get all documents from ChromaDB
            collection = self.vector_store._collection  # Accede a la colecciÃ³n de ChromaDB
            all_docs = collection.get()                 # Obtiene todos los documentos
            
            if not all_docs['documents']:  # Si no hay documentos
                return True
                
            # Find indices of placeholder documents to remove
            indices_to_remove = []  # Lista de Ã­ndices a eliminar
            for i, doc in enumerate(all_docs['documents']):  # Revisa cada documento
                if (doc and ('placeholder' in doc.lower() or   # Si contiene "placeholder"
                           'test' in doc.lower() or             # o "test" 
                           len(doc.strip()) < 10)):             # o es muy corto
                    indices_to_remove.append(i)  # Marca para eliminar
            
            # Remove placeholder documents if any found
            if indices_to_remove:  # Si hay documentos para eliminar
                ids_to_remove = [all_docs['ids'][i] for i in indices_to_remove]  # Obtiene IDs a eliminar
                collection.delete(ids=ids_to_remove)  # Elimina de ChromaDB
                print(f"Removed {len(ids_to_remove)} placeholder documents")  # Confirma eliminaciÃ³n
                return True
            
            return True  # OperaciÃ³n exitosa
        except Exception as e:
            print(f"Error cleaning placeholder documents: {e}")  # Muestra error si algo falla
            return False

def initialize_hospital_documents():
    """Inicializa con documentos mÃ©dicos y protocolos del Hospital Barros Luco"""
    # Retorna lista de protocolos mÃ©dicos, informaciÃ³n de servicios y procedimientos
    return [
        # SERVICIOS Y DEPARTAMENTOS
        "El Hospital Barros Luco ubicado en Santiago de Chile cuenta con los siguientes servicios: Emergencias 24/7, Cuidados Intensivos (UCI), CardiologÃ­a, NeurologÃ­a, PediatrÃ­a, GinecologÃ­a y Obstetricia, OncologÃ­a, Ortopedia y TraumatologÃ­a, RadiologÃ­a e ImÃ¡genes, Laboratorio ClÃ­nico, Farmacia, y RehabilitaciÃ³n FÃ­sica.",
        
        # HORARIOS Y CONTACTO
        "Horarios de atenciÃ³n: Emergencias 24 horas. Consultas externas: Lunes a Viernes 7:00 AM - 6:00 PM, SÃ¡bados 8:00 AM - 2:00 PM. TelÃ©fono principal: (01) 234-5678. Emergencias: 911. DirecciÃ³n: Av. Salud 123, Lima, PerÃº.",
        
        # PROTOCOLOS DE EMERGENCIA
        "Protocolo de emergencias: En caso de emergencia mÃ©dica, dirigirse inmediatamente al Ã¡rea de Emergencias en el primer piso. El personal de triaje evaluarÃ¡ la urgencia. CÃ³digo Azul: Paro cardiorrespiratorio. CÃ³digo Rojo: Emergencia mÃ©dica. CÃ³digo Amarillo: Emergencia quirÃºrgica.",
        
        # PROCEDIMIENTOS DE HOSPITALIZACIÃ“N
        "Proceso de hospitalizaciÃ³n: 1) AdmisiÃ³n con documento de identidad y seguro mÃ©dico. 2) EvaluaciÃ³n mÃ©dica inicial. 3) AsignaciÃ³n de habitaciÃ³n segÃºn disponibilidad. 4) Entrega de brazalete de identificaciÃ³n. 5) OrientaciÃ³n sobre normas hospitalarias. Horarios de visita: 2:00 PM - 4:00 PM y 6:00 PM - 8:00 PM.",
        
        # ESPECIALIDADES MÃ‰DICAS
        "CardiologÃ­a: DiagnÃ³stico y tratamiento de enfermedades del corazÃ³n. Servicios: Electrocardiograma, Ecocardiograma, Pruebas de esfuerzo, Cateterismo cardÃ­aco, Marcapasos. NeurologÃ­a: AtenciÃ³n de enfermedades del sistema nervioso. Servicios: Electroencefalograma, Resonancia magnÃ©tica cerebral, Tratamiento de epilepsia, ACV, migraÃ±as.",
        
        # ANÃLISIS Y LABORATORIO
        "Laboratorio ClÃ­nico: AnÃ¡lisis de sangre, orina, heces. Horario: 6:00 AM - 4:00 PM. Ayuno requerido: 12 horas para glucosa y perfil lipÃ­dico, 8 horas para quÃ­mica sanguÃ­nea. Resultados: AnÃ¡lisis bÃ¡sicos en 2-4 horas, cultivos en 48-72 horas, biopsias en 5-7 dÃ­as laborables.",
        
        # IMAGENOLOGÃA
        "Servicios de ImagenologÃ­a: Rayos X, TomografÃ­a (TAC), Resonancia MagnÃ©tica (RM), EcografÃ­a, MamografÃ­a. PreparaciÃ³n especial requerida para TAC con contraste y RM. Citas programadas de Lunes a Viernes. Urgencias atendidas las 24 horas en Emergencias.",
        
        # FARMACIA Y MEDICAMENTOS
        "Farmacia Hospitalaria: DispensaciÃ³n de medicamentos para pacientes hospitalizados y externos. Horario: 24 horas para hospitalizaciÃ³n, 7:00 AM - 8:00 PM para externos. Aceptamos recetas de mÃ©dicos colegiados. Medicamentos controlados requieren receta especial y documento de identidad.",
        
        # PEDIATRÃA
        "Servicio de PediatrÃ­a: AtenciÃ³n mÃ©dica para niÃ±os de 0 a 14 aÃ±os. Consultas programadas y emergencias pediÃ¡tricas. VacunaciÃ³n segÃºn esquema nacional. Sala de juegos disponible. AcompaÃ±ante permanente permitido. Horarios de visita especiales para familiares.",
        
        # MATERNIDAD
        "Servicio de Maternidad: Control prenatal, parto y puerperio. Sala de partos con tecnologÃ­a avanzada. Programa de preparaciÃ³n para el parto. Lactancia materna promovida. Habitaciones individuales y compartidas disponibles. Visitas del padre permitidas las 24 horas.",
        
        # POLÃTICAS DEL HOSPITAL
        "PolÃ­ticas hospitalarias: Prohibido fumar en todas las instalaciones. Uso obligatorio de mascarilla. MÃ¡ximo 2 visitantes por paciente. Silencio en Ã¡reas de hospitalizaciÃ³n. Dispositivos mÃ³viles en modo silencioso. Respeto al personal mÃ©dico y de enfermerÃ­a.",
        
        # SEGUROS Y PAGOS
        "Convenios de seguros: EsSalud, SIS, PacÃ­fico Seguros, RÃ­mac Seguros, La Positiva. Formas de pago: Efectivo, tarjetas de crÃ©dito/dÃ©bito, transferencias bancarias. FacturaciÃ³n electrÃ³nica disponible. Planes de financiamiento para cirugÃ­as mayores.",
        
        # TECNOLOGÃA E INTELIGENCIA ARTIFICIAL EN SALUD
        "El uso de IA en radiologÃ­a ayuda a detectar anomalÃ­as en imÃ¡genes mÃ©dicas con mayor precisiÃ³n. Nuestro Hospital Barros Luco utiliza sistemas de inteligencia artificial para anÃ¡lisis de radiografÃ­as, tomografÃ­as y resonancias magnÃ©ticas, lo que permite diagnÃ³sticos mÃ¡s rÃ¡pidos y exactos.",
        
        "La telemedicina permite realizar consultas mÃ©dicas a distancia, mejorando el acceso en zonas rurales. El Hospital Barros Luco ofrece servicios de teleconsulta para seguimiento de pacientes crÃ³nicos, consultas de especialidades y orientaciÃ³n mÃ©dica inicial.",
        
        "Sistemas de IA para anÃ¡lisis de laboratorio: Algoritmos que ayudan en la interpretaciÃ³n de resultados de exÃ¡menes de sangre, orina y otros fluidos corporales, reduciendo errores humanos y acelerando el proceso diagnÃ³stico.",
        
        "Asistentes virtuales mÃ©dicos: Chatbots especializados como este sistema que brindan informaciÃ³n hospitalaria 24/7, orientan sobre procedimientos y servicios, y facilitan el acceso a informaciÃ³n mÃ©dica bÃ¡sica para pacientes y familiares."
    ]

def create_medical_evaluation_dataset():
    # Crea dataset de evaluaciÃ³n sistemÃ¡tica para probar el rendimiento del sistema RAG mÃ©dico
    # Se usa en la pestaÃ±a "ğŸ§ª EvaluaciÃ³n SistemÃ¡tica" para ejecutar pruebas automÃ¡ticas de consultas mÃ©dicas
    # Mide mÃ©tricas de calidad (fidelidad, relevancia, precisiÃ³n) y genera reportes de rendimiento mÃ©dico
    # Cada elemento contiene: query (consulta mÃ©dica), expected_context (tipo de info mÃ©dica), ground_truth (respuesta mÃ©dica esperada)
    return [
        {
            "query": "Â¿CuÃ¡les son los horarios de atenciÃ³n del hospital?",
            "expected_context": "horarios de servicios hospitalarios",
            "ground_truth": "Emergencias 24 horas. Consultas externas: Lunes a Viernes 7:00 AM - 6:00 PM, SÃ¡bados 8:00 AM - 2:00 PM."
        },
        {
            "query": "Â¿QuÃ© servicios tiene el Hospital Barros Luco?",
            "expected_context": "listado de servicios mÃ©dicos",
            "ground_truth": "El hospital cuenta con Emergencias, UCI, CardiologÃ­a, NeurologÃ­a, PediatrÃ­a, GinecologÃ­a, OncologÃ­a, Ortopedia, RadiologÃ­a, Laboratorio y Farmacia."
        },
        {
            "query": "Â¿CÃ³mo es el proceso de hospitalizaciÃ³n?",
            "expected_context": "procedimientos de ingreso hospitalario",
            "ground_truth": "Requiere admisiÃ³n con documento y seguro, evaluaciÃ³n mÃ©dica, asignaciÃ³n de habitaciÃ³n, brazalete de identificaciÃ³n y orientaciÃ³n sobre normas."
        },
        {
            "query": "Â¿QuÃ© estudios se realizan en el laboratorio?",
            "expected_context": "servicios de laboratorio clÃ­nico",
            "ground_truth": "Se realizan anÃ¡lisis de sangre, orina, heces. Horario 6:00 AM - 4:00 PM. Resultados bÃ¡sicos en 2-4 horas."
        },
        {
            "query": "Â¿CuÃ¡les son los horarios de visita?",
            "expected_context": "polÃ­ticas de visitas hospitalarias",
            "ground_truth": "Horarios de visita: 2:00 PM - 4:00 PM y 6:00 PM - 8:00 PM. MÃ¡ximo 2 visitantes por paciente."
        },
        {
            "query": "Â¿CÃ³mo ayuda la IA en radiologÃ­a?",
            "expected_context": "tecnologÃ­a e inteligencia artificial en salud",
            "ground_truth": "La IA en radiologÃ­a ayuda a detectar anomalÃ­as en imÃ¡genes mÃ©dicas con mayor precisiÃ³n, permitiendo diagnÃ³sticos mÃ¡s rÃ¡pidos y exactos."
        },
        {
            "query": "Â¿QuÃ© es la telemedicina y cÃ³mo funciona?",
            "expected_context": "servicios de telemedicina",
            "ground_truth": "La telemedicina permite realizar consultas mÃ©dicas a distancia, mejorando el acceso en zonas rurales. Incluye teleconsulta, seguimiento de pacientes crÃ³nicos y orientaciÃ³n mÃ©dica."
        },
        {
            "query": "Â¿QuÃ© tecnologÃ­as de IA usa el hospital?",
            "expected_context": "sistemas de inteligencia artificial hospitalarios",
            "ground_truth": "El hospital usa IA para anÃ¡lisis radiolÃ³gico, interpretaciÃ³n de laboratorio, asistentes virtuales mÃ©dicos y sistemas de diagnÃ³stico automatizado."
        },
    ]

def export_langsmith_format(logs):
    """Exporta logs en formato LangSmith"""
    langsmith_data = []
    for log in logs:
        langsmith_data.append({
            "run_id": log['id'],
            "timestamp": log['timestamp'],
            "inputs": {"query": log['query']},
            "outputs": {"response": log['response']},
            "metrics": log['metrics'],
            "metadata": {
                "context_count": log['context_count'],
                "context_scores": log['context_scores']
            }
        })
    return langsmith_data

def main():
    # pagina streamlit 
    st.set_page_config(
        page_title="RAG Chatbot",         # TÃ­tulo de la pÃ¡gina
        page_icon="ğŸ¤–",                   # Icono del navegador
        layout="wide",                    # DiseÃ±o ancho para aprovechar pantalla
        initial_sidebar_state="expanded"  # Barra lateral expandida
    )
    
    # mero css
    st.markdown("""
    <style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);  /* Gradiente de color */
        padding: 1rem;               /* Espaciado interno */
        border-radius: 10px;         /* Bordes redondeados */
        color: white;                /* Texto blanco */
        text-align: center;          /* Centrado */
        margin-bottom: 2rem;         /* Margen inferior */
    }
    .metric-card {
        background: #f0f2f6;         /* Fondo gris claro */
        padding: 1rem;
        border-radius: 10px;
        border-left: 4px solid #667eea;  /* Borde izquierdo de color */
    }
    .chat-message {
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    .user-message {
        background: #e3f2fd;         /* Fondo azul para usuario */
        border-left: 4px solid #2196f3;
    }
    .assistant-message {
        background: #f3e5f5;         /* Fondo rosa para asistente */
        border-left: 4px solid #9c27b0;
    }
    .source-card {
        background: #fff3e0;         /* Fondo naranja claro para fuentes */
        padding: 0.8rem;
        border-radius: 8px;
        border-left: 3px solid #ff9800;  /* Borde naranja */
        margin: 0.5rem 0;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # header
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ¥ Hospital Barros Luco - Santiago de Chile - Asistente Virtual</h1>
        <p>Sistema de consultas mÃ©dicas inteligente con informaciÃ³n actualizada del hospital</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Verifica si el token de GitHub estÃ¡ disponible
    if not github_token:  # Verifica si existe el token de GitHub
        st.error("âŒ Please check your .env file and make sure GITHUB_TOKEN is set.")
        st.info("ğŸ’¡ Your .env file should contain: GITHUB_TOKEN=your_token_here")
        return

    # Inicializa el estado de la sesiÃ³n para datos persistentes
    if "chatbot_rag" not in st.session_state:  # Si es la primera vez
        st.session_state.chatbot_rag = ChatbotMedicoRAG()  # Crea instancia del chatbot mÃ©dico
        st.session_state.chatbot_rag.documents = initialize_hospital_documents()  # Carga documentos mÃ©dicos del hospital

        # Inicializa todos los componentes del sistema de forma secuencial
        if st.session_state.chatbot_rag.initialize_client():        # Inicializa cliente OpenAI
            if st.session_state.chatbot_rag.initialize_llm():       # Configura modelo de lenguaje
                if st.session_state.chatbot_rag.initialize_embeddings():  # Inicializa embeddings
                    st.session_state.chatbot_rag.setup_memory()     # Configura memoria conversacional
    
    # Estado de mensajes del chat
    if 'messages' not in st.session_state:  # Si es la primera sesiÃ³n
        st.session_state.messages = []      # Lista vacÃ­a para mensajes del chat

    # Panel lateral de control (la sidebar)
    with st.sidebar:
        st.markdown("### ğŸ› Panel de Control")  # TÃ­tulo del panel lateral
        
        # Estado del sistema
        st.markdown("#### ğŸ“Š Estado del Sistema")
        if st.session_state.chatbot_rag.embeddings is not None:  # Si los embeddings estÃ¡n listos
            st.success("âœ… Embeddings Activos")
        else:
            st.warning("âš  Embeddings Pendientes")  # Si faltan embeddings
            
        if st.session_state.chatbot_rag.documents:  # Si hay documentos cargados
            st.info(f"ğŸ“š {len(st.session_state.chatbot_rag.documents)} Documentos Cargados")
        else:
            st.error("âŒ Sin Documentos")  # Si no hay documentos
        
        # Configuraciones
        st.markdown("#### âš™ Configuraciones")
        temperature = st.slider("ğŸŒ¡ Temperatura", 0.0, 1.0, 0.1, 0.1)
        max_tokens = st.slider("ğŸ“ Max Tokens", 100, 1000, 600, 50)
        top_k = st.slider("ğŸ” Top K Docs", 1, 10, 3, 1)
        
    # Crea pestaÃ±as con diseÃ±o mejorado
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ¥ Consultas MÃ©dicas", 
        "  Protocolos y Documentos", 
        "ğŸ“Š MÃ©tricas del Sistema", 
        "ğŸ§ª EvaluaciÃ³n de Calidad", 
        "ğŸ“ˆ Reportes Hospitalarios"
    ])
    
    with tab1:
        st.markdown("### ğŸ¥ Asistente Virtual del Hospital Barros Luco")
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            # Chat interface con diseÃ±o mejorado
            chat_container = st.container()
            with chat_container:
                for message in st.session_state.messages:
                    if message["role"] == "user":
                        st.markdown(f"""
                        <div class="chat-message user-message">
                            <strong>ğŸ‘¤ Tu:</strong><br>
                            {message["content"]}
                        </div>
                        """, unsafe_allow_html=True)
                    else:
                        st.markdown(f"""
                        <div class="chat-message assistant-message">
                            <strong>ğŸ¤– Asistente:</strong><br>
                            {message["content"]}
                        </div>
                        """, unsafe_allow_html=True)
            
            # Chat input con placeholder mejorado
            if prompt := st.chat_input("ğŸ¥ PregÃºntame sobre horarios, servicios mÃ©dicos, procedimientos..."):
                # AÃ±ade mensaje del usuario
                st.session_state.messages.append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.markdown(prompt)
                
                # Genera respuesta
                with st.chat_message("assistant"):
                    with st.spinner("Procesando con mÃ©tricas..."):
                        chatbot = st.session_state.chatbot_rag
                        
                        if chatbot.embeddings is None:
                            st.warning("Genera embeddings primero en la pestaÃ±a de documentos")
                        else:
                            # BÃºsqueda hÃ­brida con mÃ©tricas
                            results, retrieval_time = chatbot.hybrid_search_with_metrics(prompt, top_k=3)
                            
                            if results:
                                # Genera respuesta
                                response, generation_time = chatbot.generate_response_with_metrics(prompt, results)
                                
                                # Calcula mÃ©tricas
                                metrics = {
                                    'retrieval_time': retrieval_time,
                                    'generation_time': generation_time,
                                    'total_time': retrieval_time + generation_time,
                                    'docs_retrieved': len(results),
                                    'avg_relevance_score': np.mean([r['combined_score'] for r in results])
                                }
                                
                                # EvalÃºa fidelidad, relevancia y precisiÃ³n del contexto
                                context_text = "\n".join([r['document'] for r in results])
                                metrics['faithfulness'] = chatbot.evaluate_faithfulness(prompt, context_text, response)
                                metrics['relevance'] = chatbot.evaluate_relevance(prompt, response)
                                metrics['context_precision'] = chatbot.evaluate_context_precision(prompt, results)
                                
                                # Muestra la respuesta
                                st.markdown(response)
                                
                                # Muestra las mÃ©tricas
                                with st.expander("ğŸ“Š MÃ©tricas de esta respuesta"):
                                    # Filtra fuentes relevantes para mÃ©tricas
                                    query_words = set(prompt.lower().split())
                                    stop_words = {'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'es', 'se', 'no', 'te', 'lo', 'le', 'da', 'su', 'por', 'son', 'con', 'para', 'como', 'quÃ©', 'cÃ³mo', 'cuÃ¡l', 'hola', 'gracias'}
                                    meaningful_words = query_words - stop_words
                                    
                                    relevant_count = 0
                                    for result in results:
                                        content = result['document'].lower()
                                        if 'placeholder' in content or 'test' in content or len(content.strip()) < 10:
                                            continue
                                        if len(meaningful_words) > 0:
                                            content_words = set(content.split())
                                            if meaningful_words.intersection(content_words):
                                                relevant_count += 1
                                    
                                    col_a, col_b, col_c, col_d = st.columns(4)
                                    with col_a:
                                        st.markdown(f"""
                                        <div class="metric-card">
                                            <h4>â± Tiempo Total</h4>
                                            <h2>{metrics['total_time']:.2f}s</h2>
                                        </div>
                                        """, unsafe_allow_html=True)
                                    with col_b:
                                        fidelity_color = "green" if metrics['faithfulness'] >= 8 else "orange" if metrics['faithfulness'] >= 6 else "red"
                                        st.markdown(f"""
                                        <div class="metric-card">
                                            <h4>ğŸ¯ Fidelidad</h4>
                                            <h2 style="color: {fidelity_color}">{metrics['faithfulness']:.1f}/10</h2>
                                        </div>
                                        """, unsafe_allow_html=True)
                                    with col_c:
                                        relevance_color = "green" if metrics['relevance'] >= 8 else "orange" if metrics['relevance'] >= 6 else "red"
                                        st.markdown(f"""
                                        <div class="metric-card">
                                            <h4>ğŸ” Relevancia</h4>
                                            <h2 style="color: {relevance_color}">{metrics['relevance']:.1f}/10</h2>
                                        </div>
                                        """, unsafe_allow_html=True)
                                    with col_d:
                                        st.markdown(f"""
                                        <div class="metric-card">
                                            <h4>ğŸ“š Docs Relevantes</h4>
                                            <h2>{relevant_count}/{metrics['docs_retrieved']}</h2>
                                        </div>
                                        """, unsafe_allow_html=True)
                                
                                # Muestra las fuentes con filtrado
                                with st.expander(f"ğŸ“š Fuentes utilizadas"):
                                    # Filtra documentos placeholder e irrelevantes
                                    query_words = set(prompt.lower().split())
                                    stop_words = {'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'es', 'se', 'no', 'te', 'lo', 'le', 'da', 'su', 'por', 'son', 'con', 'para', 'como', 'quÃ©', 'cÃ³mo', 'cuÃ¡l', 'hola', 'gracias'}
                                    meaningful_words = query_words - stop_words
                                    
                                    relevant_sources = []
                                    for result in results:
                                        content = result['document'].lower()
                                        # Omite documentos placeholder
                                        if 'placeholder' in content or 'test' in content or len(content.strip()) < 10:
                                            continue
                                        # Verifica relevancia para consultas significativas
                                        if len(meaningful_words) > 0:
                                            content_words = set(content.split())
                                            if meaningful_words.intersection(content_words):
                                                relevant_sources.append(result)
                                    
                                    if relevant_sources:
                                        st.write(f"*Documentos analizados:* {len(results)} | *Fuentes relevantes:* {len(relevant_sources)}")
                                        for i, result in enumerate(relevant_sources[:3]):  # Show only first 3 relevant sources
                                            score_color = "green" if result['combined_score'] > 0.7 else "orange" if result['combined_score'] > 0.5 else "red"
                                            st.markdown(f"""
                                            <div class="source-card">
                                                <h4>ğŸ“„ Fuente {i+1} 
                                                    <span style="color: {score_color}; font-size: 0.8em;">
                                                        (Score: {result['combined_score']:.3f})
                                                    </span>
                                                </h4>
                                                <p>{result['document'][:200]}...</p>
                                            </div>
                                            """, unsafe_allow_html=True)
                                            st.divider()
                                    else:
                                        st.info("ğŸ§  Esta respuesta se basa en conocimiento general, no en documentos especÃ­ficos de la base de datos.")
                                
                                # Registra la interacciÃ³n
                                chatbot.log_interaction(prompt, response, metrics, results)
                                
                                # AÃ±ade respuesta a la sesiÃ³n
                                st.session_state.messages.append({"role": "assistant", "content": response})
                            else:
                                st.error("Error en la bÃºsqueda de documentos")
        
        with col2:
            st.markdown("### ğŸ› Centro de Control")
            
            # Estado del sistema con indicadores
            st.markdown("#### ğŸ“Š Estado del Sistema")
            if st.session_state.chatbot_rag.embeddings is not None:
                st.success("âœ… Embeddings Activos")
            else:
                st.warning("âš  Embeddings Pendientes")
                
            if st.session_state.chatbot_rag.documents:
                st.info(f"ğŸ“š {len(st.session_state.chatbot_rag.documents)} Documentos")
            else:
                st.error("âŒ Sin Documentos")
            
            st.markdown("#### ğŸ”§ Acciones RÃ¡pidas")
            
            if st.button("ğŸ—‘ Limpiar Chat", use_container_width=True):
                st.session_state.messages = []
                if st.session_state.chatbot_rag.memory:
                    st.session_state.chatbot_rag.memory.clear()
                st.rerun()
            
            if st.button("ğŸ”„ Generar Embeddings"):
                if st.session_state.chatbot_rag.documents and st.session_state.chatbot_rag.embeddings_model:
                    with st.spinner("Generando embeddings..."):
                        embeddings = st.session_state.chatbot_rag.get_embeddings_langchain(
                            st.session_state.chatbot_rag.documents
                        )
                        if embeddings is not None:
                            st.session_state.chatbot_rag.embeddings = embeddings
                            st.success("âœ… Embeddings listos")
                        else:
                            st.error("âŒ Error generando embeddings")
                else:
                    st.warning("Documentos no disponibles")
            
            
            # EstadÃ­sticas rÃ¡pidas
            st.markdown("####   EstadÃ­sticas RÃ¡pidas")
            if st.session_state.chatbot_rag.interaction_logs:
                total_interactions = len(st.session_state.chatbot_rag.interaction_logs)
                avg_time = np.mean([log['metrics']['total_time'] for log in st.session_state.chatbot_rag.interaction_logs])
                st.metric("  Conversaciones", total_interactions)
                st.metric("â± Tiempo Promedio", f"{avg_time:.2f}s")
            else:
                st.info("ğŸ“Š Sin datos aÃºn")
    
    with tab2:
        st.header("ğŸ“„ GestiÃ³n de Documentos")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("ğŸ“š Documentos Actuales")
            
            for i, doc in enumerate(st.session_state.chatbot_rag.documents):
                with st.expander(f"Documento {i+1} ({len(doc)} caracteres)"):
                    st.text_area(
                        f"Contenido:",
                        value=doc,
                        height=100,
                        key=f"doc_display_{i}",
                        disabled=True
                    )
                    
                    col_edit, col_delete = st.columns(2)
                    with col_edit:
                        if st.button(f"âœ Editar", key=f"edit_{i}"):
                            st.session_state[f'editing_doc_{i}'] = True
                    
                    with col_delete:
                        if st.button(f"ğŸ—‘ Eliminar", key=f"delete_{i}"):
                            st.session_state.chatbot_rag.documents.pop(i)
                            st.session_state.chatbot_rag.embeddings = None
                            st.rerun()
                    
                    # EdiciÃ³n online del contenido
                    if st.session_state.get(f'editing_doc_{i}', False):
                        new_content = st.text_area(
                            "Editar contenido:",
                            value=doc,
                            height=150,
                            key=f"edit_content_{i}"
                        )
                        
                        col_save, col_cancel = st.columns(2)
                        with col_save:
                            if st.button(f"ğŸ’¾ Guardar", key=f"save_{i}"):
                                st.session_state.chatbot_rag.documents[i] = new_content
                                st.session_state[f'editing_doc_{i}'] = False
                                st.session_state.chatbot_rag.embeddings = None
                                st.success("Documento actualizado")
                                st.rerun()
                        
                        with col_cancel:
                            if st.button(f"âŒ Cancelar", key=f"cancel_{i}"):
                                st.session_state[f'editing_doc_{i}'] = False
                                st.rerun()
        
        with col2:
            st.subheader("â• Agregar Documento")
            
            new_doc = st.text_area(
                "Contenido del nuevo documento:",
                height=200,
                placeholder="Escribe aquÃ­ el contenido del nuevo documento..."
            )
            
            if st.button("ğŸ“ Agregar Documento"):
                if new_doc.strip():
                    st.session_state.chatbot_rag.documents.append(new_doc.strip())
                    st.session_state.chatbot_rag.embeddings = None
                    st.success("Documento agregado exitosamente")
                    st.rerun()
                else:
                    st.warning("El documento no puede estar vacÃ­o")
            
            st.subheader("ğŸ“Š EstadÃ­sticas")
            st.metric("Total documentos", len(st.session_state.chatbot_rag.documents))
            
            if st.session_state.chatbot_rag.documents:
                avg_length = np.mean([len(doc) for doc in st.session_state.chatbot_rag.documents])
                st.metric("Longitud promedio", f"{avg_length:.0f} caracteres")
                
                total_words = sum([len(doc.split()) for doc in st.session_state.chatbot_rag.documents])
                st.metric("Total palabras", f"{total_words:,}")
    
    with tab3:
        st.header("ğŸ“Š Dashboard de MÃ©tricas")
        
        if st.session_state.chatbot_rag.interaction_logs:
            df = pd.DataFrame([
                {
                    'timestamp': log['timestamp'],
                    'query_length': len(log['query']),
                    'response_length': len(log['response']),
                    **log['metrics']
                }
                for log in st.session_state.chatbot_rag.interaction_logs
            ])
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("â± Tiempo de Respuesta")
                fig = px.line(df, x='timestamp', y='total_time', title="Tiempo Total por Consulta")
                st.plotly_chart(fig, use_container_width=True)
                
                st.subheader("ğŸ“Š DistribuciÃ³n de Fidelidad")
                fig = px.histogram(df, x='faithfulness', title="DistribuciÃ³n de Puntuaciones de Fidelidad")
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.subheader("ğŸ” MÃ©tricas de RecuperaciÃ³n")
                fig = px.scatter(df, x='retrieval_time', y='generation_time', 
                               size='docs_retrieved', title="Tiempo de RecuperaciÃ³n vs GeneraciÃ³n")
                st.plotly_chart(fig, use_container_width=True)
                
                st.subheader("ğŸ¯ Calidad vs PrecisiÃ³n")
                fig = px.scatter(df, x='context_precision', y='relevance', 
                               title="PrecisiÃ³n de Contexto vs Relevancia")
                st.plotly_chart(fig, use_container_width=True)
            
            st.subheader("ğŸ“ˆ EstadÃ­sticas Generales")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Total Consultas", len(df))
            with col2:
                st.metric("Tiempo Promedio", f"{df['total_time'].mean():.2f}s")
            with col3:
                st.metric("Fidelidad Promedio", f"{df['faithfulness'].mean():.1f}/10")
            with col4:
                st.metric("Relevancia Promedio", f"{df['relevance'].mean():.1f}/10")
        else:
            st.info("No hay datos de interacciones aÃºn. Realiza algunas consultas primero.")
    
    with tab4:
        st.header("ğŸ§ª EvaluaciÃ³n SistemÃ¡tica")
        
        if st.button("ğŸ§ª Ejecutar EvaluaciÃ³n Completa"):
            if st.session_state.chatbot_rag.embeddings is None:
                st.warning("Genera embeddings primero")
            else:
                eval_dataset = create_medical_evaluation_dataset()
                results = []
                
                with st.spinner("Ejecutando evaluaciÃ³n sistemÃ¡tica..."):
                    for test_case in eval_dataset:
                        query = test_case['query']
                        
                        docs, retrieval_time = st.session_state.chatbot_rag.hybrid_search_with_metrics(query, 3)
                        
                        if docs:
                            response, generation_time = st.session_state.chatbot_rag.generate_response_with_metrics(query, docs)
                            
                            context_text = "\n".join([d['document'] for d in docs])
                            faithfulness = st.session_state.chatbot_rag.evaluate_faithfulness(query, context_text, response)
                            relevance = st.session_state.chatbot_rag.evaluate_relevance(query, response)
                            context_precision = st.session_state.chatbot_rag.evaluate_context_precision(query, docs)
                            
                            results.append({
                                'query': query,
                                'response': response[:100] + "...",
                                'retrieval_time': retrieval_time,
                                'generation_time': generation_time,
                                'faithfulness': faithfulness,
                                'relevance': relevance,
                                'context_precision': context_precision,
                                'ground_truth': test_case['ground_truth'][:100] + "..."
                            })
                
                if results:
                    st.subheader("ğŸ“Š Resultados de EvaluaciÃ³n")
                    eval_df = pd.DataFrame(results)
                    st.dataframe(eval_df)
                    
                    st.subheader("ğŸ“ˆ MÃ©tricas Promedio")
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("Fidelidad", f"{eval_df['faithfulness'].mean():.1f}/10")
                    with col2:
                        st.metric("Relevancia", f"{eval_df['relevance'].mean():.1f}/10")
                    with col3:
                        st.metric("PrecisiÃ³n", f"{eval_df['context_precision'].mean():.2f}")
                    with col4:
                        st.metric("Tiempo total", f"{(eval_df['retrieval_time'] + eval_df['generation_time']).mean():.2f}s")
    
    with tab5:
        st.header("ğŸ“ˆ Analytics y ExportaciÃ³n")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“¤ Exportar Datos")
            
            if st.button("ğŸ“Š Exportar para LangSmith"):
                if st.session_state.chatbot_rag.interaction_logs:
                    langsmith_data = export_langsmith_format(st.session_state.chatbot_rag.interaction_logs)
                    st.json(langsmith_data[:2])  # Preview
                    
                    json_str = json.dumps(langsmith_data, indent=2, ensure_ascii=False)
                    st.download_button(
                        label="ğŸ’¾ Descargar JSON LangSmith",
                        data=json_str,
                        file_name=f"langsmith_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json"
                    )
                else:
                    st.info("No hay datos para exportar")
            
            if st.button("ğŸ“Š Exportar CSV"):
                if st.session_state.chatbot_rag.interaction_logs:
                    df = pd.DataFrame([
                        {
                            'timestamp': log['timestamp'],
                            'query': log['query'],
                            'response': log['response'][:100] + "...",
                            **log['metrics']
                        }
                        for log in st.session_state.chatbot_rag.interaction_logs
                    ])
                    
                    csv = df.to_csv(index=False)
                    st.download_button(
                        label="ğŸ’¾ Descargar CSV",
                        data=csv,
                        file_name=f"rag_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                else:
                    st.info("No hay datos para exportar")
        
        with col2:
            st.subheader("ğŸ“Š AnÃ¡lisis de Documentos")
            
            if st.session_state.chatbot_rag.documents:
                # Document statistics
                doc_lengths = [len(doc) for doc in st.session_state.chatbot_rag.documents]
                
                fig = px.bar(
                    x=list(range(1, len(doc_lengths) + 1)),
                    y=doc_lengths,
                    title="DistribuciÃ³n de Longitud de Documentos",
                    labels={'x': 'ID Documento', 'y': 'Caracteres'}
                )
                st.plotly_chart(fig, use_container_width=True)

                # Frecuencia de palabras
                all_text = " ".join(st.session_state.chatbot_rag.documents)
                words = all_text.lower().split()
                word_freq = {}
                for word in words:
                    if len(word) > 3:  # Filtra palabras cortas
                        word_freq[word] = word_freq.get(word, 0) + 1
                
                top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
                
                if top_words:
                    fig = px.bar(
                        x=[word[0] for word in top_words],
                        y=[word[1] for word in top_words],
                        title="Top 10 Palabras MÃ¡s Frecuentes"
                    )
                    st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("No hay documentos disponibles para anÃ¡lisis")

if __name__ == "__main__":
    main()